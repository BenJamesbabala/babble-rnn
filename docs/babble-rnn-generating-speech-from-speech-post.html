<!DOCTYPE html>
<html>
<head>
	<meta content="text/html; charset=utf-8" http-equiv="content-type">
	<style type="text/css">

   body {
     font-family: sans-serif; 
     color: #555; 
     font-size: 14pt;
   }

   a { 
     color: #42b3f4 !important;
   }
   .article {
     padding:30pt 20pt; max-width:451.4pt; margin: 0 auto;
   
   }
   
	</style>
	
	<title>Babble-rnn: Generating speech from speech with LSTM networks</title>
</head>
<body style="">
<div class="article">
	<p class="title" id="top" style="padding-top:0pt;margin:0;padding-left:0;font-size:26pt;padding-bottom:3pt;line-height:1.15;page-break-after:avoid;font-family:&quot;Arial&quot;;orphans:2;widows:2;text-align:left;padding-right:0"><span style="font-weight:400;text-decoration:none;vertical-align:baseline;font-size:26pt;font-family:&quot;Arial&quot;;font-style:normal">Babble-rnn: Generating speech from speech with LSTM networks</span></p>
	<p>&nbsp;</p>
	<p style="padding:0;margin:0;;line-height:1.15;orphans:2;widows:2;text-align:right"><span style="font-weight:700;text-decoration:none;vertical-align:baseline;;font-style:normal">Phil Ayres</span></p>
	<p style="padding:0;margin:0;;line-height:1.15;orphans:2;widows:2;text-align:right"><span style="font-weight:700;text-decoration:none;vertical-align:baseline;;font-style:normal">phil.ayres@consected.com</span></p>
	<p style="padding:0;margin:0;;line-height:1.15;orphans:2;widows:2;text-align:right"><span style="font-weight:700;text-decoration:none;vertical-align:baseline;;font-style:normal">25 May 2017</span></p>
	
	<p>There is plenty of interest in recurrent neural networks (RNNs) for the generation of data that is meaningful, and even fascinating to humans. Popular examples generate everything from credible (but fabricated) passages from Shakespeare, incredible (but highly likely) fake-news clickbait, to completely simulated handwritten sentences that shadow the style of the original writer.</p>
	
	<p>These examples by prominent authors have helped many people, including myself, to see that there is more to deep-learning than just recognising cats and dogs in social media photos:</p>
	
	<ul class="lst-kix_8xo8w4yl2i0y-0 start">
		<li><a href="http://karpathy.github.io/2015/05/21/rnn-effectiveness/">The Unreasonable Effectiveness of Recurrent Neural Networks</a><br>
		Andrej Karpathy</li>
		<li><a href="https://arxiv.org/abs/1308.0850">Generating Sequences With Recurrent Neural Networks</a><br>
		Alex Graves</li>
		<li><a href="https://larseidnes.com/2015/10/13/auto-generating-clickbait-with-recurrent-neural-networks/">Auto-Generating Clickbait With Recurrent Neural Networks</a><br>
		Lars Eidnes</li>
	</ul>
	
	<p>Such compelling examples demonstrate the power of recurrent neural networks for modelling human generated data, and recreating completely new data from those models. They can be understood by anybody, humanising the world of &ldquo;AI&rdquo;.</p>
	
	<p>Inspired, <a href="https://www.consected.com/">Consected</a>, sponsored research into the use of machine learning to generate new speech by modelling human speech audio, without any intermediate text or word representations. The idea is to learn to speak through imitation, much like a baby might. The goal is to generate a babbling audio output that emulates the speech patterns of the original speaker, ideally incorporating real words into the output.</p>
	
	<p>A requirement of this project is to evaluate the use of commonly researched neural network architectures that can be implemented directly with the <a href="https://keras.io/">Keras</a>&nbsp;toolkit. Rather than merely being a simple implementation of existing research, our project investigates the use of an alternative form of audio encoding, which we are not aware of being used previously in machine learning. We also trial various training regimes to attempt to achieve faster or improved convergence of the model.</p>
	
	<p>Of course, the <a href="https://github.com/philayres/babble-rnn">source code</a> for the networks, generated models, plus Jupyter / iPython notebooks acting as a training / testing dashboard are provided for reference.</p>
	
	<p>If you want to skip the details and just hear the results, jump to <a href="#additionalresults">Additional Generated Audio</a></p>
	
	<h2 id="h.ltylc9r84smm">Audio Generation Research</h2>
	<p>The existing research we have found related to audio generation with RNNs has provided some background to the problem and possible neural network architectures. These papers have taken various approaches to the actual representation of audio data or are focused on a more traditional text-to-speech problem. Key references are:</p>
	
	<ul class="lst-kix_4pukr7vfg0d0-0 start">
		<li><a href="http://www.johnglover.net/blog/generating-sound-with-rnns.html">Generating sound with recurrent neural networks</a><br>
		John Glover</li>
		<li><a href="https://research.google.com/pubs/pub43893.html">Acoustic Modeling in Statistical Parametric Speech Synthesis - from HMM to LSTM-RNN</a><br>
		Heiga Zen</li>
		<li><a href="https://research.google.com/pubs/archive/38131.pdf">Deep Neural Networks for Acoustic Modeling in Speech Recognition</a><br>
		Geoffrey Hinton, Li Deng, Dong Yu, George Dahl, Abdel-rahman Mohamed, Navdeep Jaitly, Andrew Senior, Vincent Vanhoucke, Patrick Nguyen, Tara Sainath, and Brian Kingsbury</li>
		<li><a href="https://cs224d.stanford.edu/reports/NayebiAran.pdf">GRUV:Algorithmic Music Generation using Recurrent Neural Networks</a><br>
		Aren Nayebi and Matt Vitelli</li>
	</ul>
	<p><i>Note: a list of <a href="#h.fcgggf9hdrrr">all references</a> appears towards the end of this document</a></i> </p>
	<h2 id="h.v4y4okos56m9">Encoding Audio</h2>
	<p>A significant hurdle in the generation of audio by a neural network is the learning of audio sequences over significant periods of time, when faced with the amount of data required to represent a coherent audio sequence.</p>
	
	<p>When attempting to learn a model with a wide-spectrum waveform (CD quality) directly from the pulse coded modulation (PCM) digitised representation of the audio, the network would be required to learn a sequence of 44,100 samples of 16-bit data for a single second of audio. Add to this the desire to generate more than just a short burst of audio, and raw digitised audio data seems like an unrealistic source for learning. Surprisingly, Nayebi and Vitelli suggest that their GRUV network could generate some coherent audio despite this.</p>
	
	<p>John Glover took a potentially more feasible approach when generating musical instrument sounds, by relying on a Phase Vocoder representing audio as short-time Fourier transforms. This represents the audio data in a significantly compressed form, reducing the length of sequences to be learned significantly. My observation would be that this is potentially a more meaningful audio representation. Rather than pushing continuous amplitude variations (which are highly affected by volume, noise and timing for even simple waveforms) directly into an RNN, sets of frequency and phase values are instead used. I look at this as the difference between an RNN learning and generating text using ASCII characters, rather than attempting to model raw bit streams from the text corpus.</p>
	
	<p>For our research, I was hoping to rely on a more specialised encoder, specifically focused on speech data. Initially I considered the GSM &ldquo;mobile phone&rdquo; standard, recalling that its compression was based on vocal tract modelling. Looking at the <a href="http://www.etsi.org/deliver/etsi_gts/06/0610/03.02.00_60/gsmts_0610sv030200p.pdf">GSM 06.10 &ldquo;Full Rate Speech Transcoding&rdquo;</a>&nbsp;standard, I observed that the complexity of the codec was much greater than anticipated, and the data contained in each frame was less representative of a short segment of audio than expected.</p>
	
	<p>This led my to search for other encoding approaches. I fortunately found <a href="http://www.rowetel.com/?page_id=452">Codec 2</a>, an open source model for digitising speech audio for broadcast over HF/VHF and ham radio. The codec uses harmonic sinusoidal coding tuned to speech. In summary, it encodes a combination of primary pitch and energy of an audio frame, followed by related harmonics, plus some flags indicating voiced (vowels), or unvoiced (consonants). The codec importantly captures the fundamentals of vocal audio as individual parameters for short audio segments (see <a href="#h.gfev4o4f20r1">Codec 2 Data Frame</a>&nbsp;for details).</p>
	<p>A major appeal of Codec 2 is that the harmonic sinusoidal coding relates all encoded harmonic components back to the primary frequency. In this way, it is expected that similar phonetic sounds (phones being the more correct terminology) made by different speakers at different pitches are more likely to appear related in data.</p>
	
	<p>Another appeal of Codec 2 for this research is that it utilises a vocal model for compression and a data representation that maintains the underlying audio components. In the 1300bps version of the codec a single 16 parameter frame represents 40ms of audio, such that just 25 frames are required per second (a rate of 'symbols' to be learned of approximately <span style="vertical-align:super">1</span>/<span style="vertical-align:sub">1700</span>&nbsp;of 44.1kbps PCM), with more meaningful vocal representation. Based on this, it was assumed that an RNN could likely model audio sequences directly from the data, rather than requiring additional preprocessing or convolutional layers.</p>
	
	<p>In order to validate the likelihood of the model making reasonable predictions from one frame to the next, a rapid analysis of <a href="#h.1c0d9jetrhe2">Codec 2 Parameter Progression</a>&nbsp;was made, to show the spread of parameters, delta from one frame to the next, and likely sensitivity to loss in the model.</p>
	<h2 id="h.to7lw5gpf56">Neural Network Architecture</h2>
	<p>The neural network architecture used in this project is quite common in the literature over the last few years. I selected it based on research of the most effective RNN architectures for sequence generation that can also be implemented using standard layers in the <a href="https://keras.io/">Keras</a>&nbsp;toolset. Simply described, the network takes Codec 2 encoded audio as its input, and utilises three long short-term memory LSTM layers (<a href="#h.fcgggf9hdrrr">ref: 10</a>) with a final fully connected rectified linear unit (ReLU) layer. In the figure below showing the architecture, the top-most orange components are used only during generation of new speech samples.</p>
	
	
	<p style="padding:0;margin:0;;line-height:1.15;orphans:2;widows:2;text-align:center"><span style="color:#ff9900;">predicted Codec 2 encoded audio</span></p>
	<p style="padding:0;margin:0;;line-height:1.15;orphans:2;widows:2;text-align:center"><span style="color:#ff9900;">|</span></p>
	<p style="padding:0;margin:0;;line-height:1.15;orphans:2;widows:2;text-align:center"><span style="color:#ff9900">un</span><span style="color:#ff9900">scale Codec 2 frame data</span></p>
	<p style="padding:0;margin:0;;line-height:1.15;orphans:2;widows:2;text-align:center"><span style="color:#ff9900;">|</span></p>
	<p style="padding:0;margin:0;;line-height:1.15;orphans:2;widows:2;text-align:center"><span style="font-style:italic">output: 16 elements</span></p>
	<p style="padding:0;margin:0;;line-height:1.15;orphans:2;widows:2;text-align:center"><span>|</span></p>
	<p style="padding:0;margin:0;;line-height:1.15;orphans:2;widows:2;text-align:center"><span>[</span><span style="font-weight:700">Dense 1</span><span>: 16 ReLU units]</span></p>
	<p style="padding:0;margin:0;;line-height:1.15;orphans:2;widows:2;text-align:center"><span>|</span></p>
	<p style="padding:0;margin:0;;line-height:1.15;orphans:2;widows:2;text-align:center"><span>[</span><span style="font-weight:700">LSTM 3</span><span>]</span></p>
	<p style="padding:0;margin:0;;line-height:1.15;orphans:2;widows:2;text-align:center"><span>|</span></p>
	<p style="padding:0;margin:0;;line-height:1.15;orphans:2;widows:2;text-align:center"><span>[</span><span style="font-weight:700">LSTM 2</span><span>]</span></p>
	<p style="padding:0;margin:0;;line-height:1.15;orphans:2;widows:2;text-align:center"><span>|</span></p>
	<p style="padding:0;margin:0;;line-height:1.15;orphans:2;widows:2;text-align:center"><span>[</span><span style="font-weight:700">LSTM 1</span><span>]</span></p>
	<p style="padding:0;margin:0;;line-height:1.15;orphans:2;widows:2;text-align:center"><span>|</span></p>
	<p style="padding:0;margin:0;;line-height:1.15;orphans:2;widows:2;text-align:center"><span style="font-weight:400;text-decoration:none;vertical-align:baseline; font-style:italic">input: 16 elements</span></p>
	<p style="padding:0;margin:0;;line-height:1.15;orphans:2;widows:2;text-align:center"><span style="font-weight:400;text-decoration:none;vertical-align:baseline;">|</span></p>
	<p style="padding:0;margin:0;;line-height:1.15;orphans:2;widows:2;text-align:center"><span style="font-style:italic">scale Codec 2 frame data</span></p>
	<p style="padding:0;margin:0;;line-height:1.15;orphans:2;widows:2;text-align:center"><span style="font-weight:400;text-decoration:none;vertical-align:baseline;">|</span></p>
	<p style="padding:0;margin:0;;line-height:1.15;orphans:2;widows:2;text-align:center"><span style="font-style:italic">Codec 2 encoded audio</span></p>
	
	<p>Prior to training the network, the raw audio data is encoded as a Codec 2 file (MP3 to raw PCM having already been performed), where each frame of data consists of 16 parameters of one (unsigned) byte each. Scaling of the Codec 2 frames takes each parameter and scales it to a floating point number between 0 and 1, based on a constant scaling factor for each parameter. Each scaled frame of 16 values now represents a single timestep input to <span style="font-weight:700">LSTM 1</span>.</p>
	
	<p>Since our network is attempting to return real values rather than being structured for classification tasks with a final softmax layer (<a href="#h.fcgggf9hdrrr">ref: 11</a>), it culminates in a single fully connected ReLU layer (named <span style="font-weight:700">Dense 1</span>) on the output of the three layers of LSTMs. ReLU was selected based on its relative simplicity, a good fit to the data (positive, real numbers) and successful application in previous research.</p>
	<h2 id="h.100ggq4b6k7j">Training the Network</h2>
	<p>Various layer sizes, optimizers and training regimes were tested early on in the project. These included:</p>
	
	<ul class="lst-kix_9964vpb2d50w-0 start">
		<li>one, two or three LSTM layers</li>
		<li>short or long LSTM layers</li>
		<li>GRU units</li>
		<li>optimisers including SGD, RMSprop, Adam</li>
		<li>multiple fully connected layers on top of the LSTM layers</li>
		<li>a fully connected layer in between each LSTM</li>
		<li>combinations of trainable and untrainable layers for several epochs</li>
	</ul>
	
	<p>Three layers of LSTMs appears to be a common architecture for generator RNNs in the literature, and our preliminary testing confirmed this with medium-length LSTM layers. With a longer LSTM (320 units), two layers performed as effectively as three layers of 160 units. Short layers (40 units each) performed poorly, as did a rapid test with GRUs rather than LSTM units.</p>
	
	<p>Similar to Graves, we found that it was necessary to train LSTM layers from the bottom up. Intuitively this allows the lowest level LSTM to achieve its own convergence rapidly, then allow the higher layers to build off of this. Attempting to train all three LSTMs simultaneously led to a sub-optimal model.</p>
	
	<p>Our eventual regime trained <span style="font-weight:700">LSTM 1</span> for 60 epochs, <span style="font-weight:700">LSTM 1</span>&nbsp;and <span style="font-weight:700">LSTM 2</span>&nbsp;for the next 180 epochs, and <span style="font-weight:700">LSTM 3</span>&nbsp;only thereafter. The number of epochs selected for each breakpoint was based purely on observation of loss during initial tests. In future research we might evaluate this approach in more detail</p>
	
	<p>The optimiser we found most appropriate was Nadam, the version of Adam with Nesterov momentum (<a href="#h.fcgggf9hdrrr">ref: 12</a>). This optimiser allowed us to focus on adjusting other architectural variables, rather than fine tuning the learning rate hyperparameter.</p>
	
	<p>Losses measured during learning were handled by scaling each Codec 2 parameter back to its real scale prior to loss calculation. This allowed a one bit error in fundamental frequency (<span style="font-style:italic">W</span><span style="vertical-align:sub;font-style:italic">0</span>) for example to be considered equivalent in impact to a one bit error in a &ldquo;voiced&rdquo; flag, rather than <span style="vertical-align:super">1</span><span style="font-style:italic">/</span><span style="vertical-align:sub">128</span> if measuring loss on the 0-1 scaled data.</p>
	<h2 id="h.grp4onuyqofv">Corpus</h2>
	<p>Training our network only requires an extended set of clear speech, and does not need other tagging or classification. <a href="http://audio.verkaro.org/">Verkaro Audiobooks</a>&nbsp;was the source of our corpus: <a href="http://audio.verkaro.org/audiobook/A-Tale-of-Two-Cities/">A Tale of Two Cities</a>. A narration with a female voice was selected, in order to be clearer under heavy compression.</p>
	
	<p>The chapters were concatenated to provide about 16 hours of speech from a single performer. The audio was converted to 8kbps 16-bit raw PCM data prior to Codec 2 encoding. The total length is approximately 1.5 million 16 byte Codec 2 frames after conversion.</p>
	
	<p>Training used a batch length of 200 frames, equivalent to 8 seconds of speech, overlapping so that batches were picked starting every 20 frames.</p>
	<h2 id="h.b6162exy2iq0">Sample Generation</h2>
	<p>The proof of the performance of the <span style="font-weight:700">babble-rnn</span> is less about the measured loss and more in the observed quality of audio produced. Sample generation uses feed-forward of the network, a frame at a time, based on the Keras example LSTM text generation procedure:</p>
	
	<ul class="lst-kix_2x1png4tdnmy-0 start">
		<li>a seed sequence is passed to the network, and a prediction sample is returned</li>
		<li>the seed sequence is updated to append the new sample and push the initial value out of the list, maintaining a constant length</li>
		<li>the new sequence is passed as the seed to the network</li>
		<li>the procedure is repeated until a desired length has been fulfilled</li>
	</ul>
	
	<p>Since the network prediction returns a full Codec 2 frame as the 16 output values, the network effectively generates 40ms of audio per sample.</p>
	
	<h2 id="h.mpz3zqpcr82z">Results</h2>
	<p>We generate 400 samples per test periodically through the training process. To evaluate performance we view both charts of Codec 2 data and PCM waveforms, and listen to generated audio. From an audible perspective we are looking for generated audio that follow the pace of the original narration, a natural pitch inflection and some proximity of tonal sequences to real words.</p>
	
	<p>The overall loss the model converges to while learning is not entirely meaningful (there is no &lsquo;correct&rsquo; answer); despite this the results are interesting. The best mean absolute error per frame we achieved is close to 0.15 at best. Intuitively this suggests that next sample predictions are close to the required fundamental frequency, harmonics and voicing, since each frame parameter is an integer.</p>
	
	<p>Unfortunately, short LSTMs (40 units each) appear to be unable to converge on a meaningful result, instead producing a frame sequence of one continuous pitch, or a pulsating tone. Intuitively, the lowest LSTM layer may be unable to model sufficient variation in frame sequences, settling instead on a generalised but insufficiently complex output.</p>
	
	<p>Medium sized LSTMs (160 units) converge on a mean absolute loss of about 0.55. </p>
	<p style="text-align:center"><img src="160-loss.png" /></p>

	<p>The audio generated does manage to achieve some structure that has sequences of roughly word length. The pitch was variable and vaguely natural. Overfitting does not appear to be an issue.</p>
	
	<p><span style="font-style:italic">Note: in the following examples, the initial 64000 samples (8 seconds of audio or 200 Codec 2 frames) are the seed sequence, included for reference. A plot of the generated audio waveform, and scaled plot of Codec 2 frame parameters are also shown to highlight the data being generated. </p>
	
	<p style="padding:0;margin:0;;line-height:1.15;orphans:2;widows:2;text-align:center">
	<audio controls>
 <source src="160-3.wav" type="audio/wav">
 Your browser does not support the audio tag.
</audio>
<img src="160-3.png" /> <img src="160-3-c2.png" />
	</p>
	
	
	<p>Since training is performed from lowest to highest layers, it is instructive to also hear the results earlier in the training.</p>
	
	<p>First layer trained:</p>
	<p style="text-align:center">
	<audio controls>
 <source src="160-1.wav" type="audio/wav">
 Your browser does not support the audio tag.
</audio>
	</p>
	<p style="text-align:center"><img src="160-1.png" /><img src="160-1-c2.png" />
	</p>
	

	<p style="padding:0;margin:0;;line-height:1.15;orphans:2;widows:2;height:11pt;text-align:center">&nbsp;</p>
	<p>Second layer trained:</p>
	
	<p style="text-align:center">
	<audio controls>
 <source src="160-2.wav" type="audio/wav">
 Your browser does not support the audio tag.
</audio>
	</p>
	<p style="text-align:center"><img src="160-2.png" /><img src="160-2-c2.png" />
	</p>

	
	<p>Long LSTMs (320 units) converge on a mean absolute loss around 0.15.</p>
 <p style="text-align:center"><img src="320-loss.png" /></p>
	
	<p>The audio generated manages word and sentence length sequences. Occasional babblings vary in pitch wildly, and overall the result seems maybe less comprehensible than the shorter 160 unit LSTM layer results.</p>
	
	<p style="text-align:center">
	<audio controls>
 <source src="320-3.wav" type="audio/wav">
 Your browser does not support the audio tag.
</audio>
	</p>
	<p style="text-align:center"><img src="320-3.png" /><img src="320-3-c2.png" /></p>
	

<p><a href="#additionalresults">Additional Generated Audio</a> provides longer sequences that allow additional comparison of the results. In these, the 320 unit LSTMs perform a little better to my ear.</p>	
	<h2 id="h.dv78nfegys7">Conclusions</h2>
	<p>A neural network architecture of deep LSTM layers has been shown to be able to learn to model speech-based audio when encoded by a codec designed specifically for vocal compression. When run as a feed-forward generator, the network manages to predict samples that track the original pace of the vocal performer with natural sounding pitch inflections over the course of sentences, and short term sequences that might reflect the structure of words.</p>
	
	<p>Although we were unable to achieve a model that actually generated true words, the model could be considered to babble in a reasonably acceptable way.</p>
	<h2 id="h.g8zy1pzg73ej">Opportunities for Future Research</h2>
	<p>Listening to the corpus with the current Codec 2 compression rate (1300bps) reveals that the compression obscures many vocal phones significantly. In future we would attempt training a similar network with a higher bit-rate version of Codec 2. Although the codec data will be higher resolution and therefore may require more effort to learn, it is also possible that the resolution of data could result in less confusion between similar sounds and therefore make it easier for the model to flow to a more coherent next sound during prediction. From a qualitative standpoint, it is likely to also be better for human evaluation to understand the results.</p>
	
	
	<p>Intuitively, it seems that the ability to evaluate loss not just on the next frame, but several frames  ahead could help guide learning more rapidly. Although this may be considered double dipping from a formal standpoint, and increase complexity for learning with back propogation through time, understanding both the theoretical and practical implications of this deserve additional research.</p>
	
	<p>Application of alternative neural network architectures should be considered in the future. Mixture Density Networks as used in Graves handwriting generation (ref: 2) and Glover musical instrument sound generation (ref: 4), based on the paper by Bishop (ref:15), seems to be worth evaluation. Similarly, HyperLSTM (ref: 14) and Diagonal LSTM (ref: 9) have shown some success in sequence generation and we would consider future research with these structures.</p>
	
	<h2 id="h.fcgggf9hdrrr">All References</h2>
	<ol class="lst-kix_spfx41keudyh-0 start" start="1">
		<li><a href="https://www.google.com/url?q=http://karpathy.github.io/2015/05/21/rnn-effectiveness/">The Unreasonable Effectiveness of Recurrent Neural Networks</a><br>
		Andrej Karpathy</li>
		<li><a href="https://www.google.com/url?q=https://arxiv.org/abs/1308.0850">Generating Sequences With Recurrent Neural Networks</a><br>
		Alex Graves</li>
		<li><a href="https://www.google.com/url?q=https://larseidnes.com/2015/10/13/auto-generating-clickbait-with-recurrent-neural-networks/">Auto-Generating Clickbait With Recurrent Neural Networks</a><br>
		Lars Eidnes</li>
		<li><a href="https://www.google.com/url?q=http://www.johnglover.net/blog/generating-sound-with-rnns.html">Generating sound with recurrent neural networks</a><br>
		John Glover</li>
		<li><a href="https://www.google.com/url?q=https://research.google.com/pubs/pub43893.html">Acoustic Modeling in Statistical Parametric Speech Synthesis - from HMM to LSTM-RNN</a><br>
		Heiga Zen</li>
		<li><a href="https://www.google.com/url?q=https://research.google.com/pubs/archive/38131.pdf">Deep Neural Networks for Acoustic Modeling in Speech Recognition</a><br>
		Geoffrey Hinton, Li Deng, Dong Yu, George Dahl, Abdel-rahman Mohamed, Navdeep Jaitly, Andrew Senior, Vincent Vanhoucke, Patrick Nguyen, Tara Sainath, and Brian Kingsbury</li>
		<li><a href="https://www.google.com/url?q=https://cs224d.stanford.edu/reports/NayebiAran.pdf">GRUV:Algorithmic Music Generation using Recurrent Neural Networks</a><br>
		Aren Nayebi and Matt Vitelli</li>
		<li><a href="https://www.google.com/url?q=http://www.cc.gatech.edu/~hays/7476/projects/Avery_Wenchen/">Generative Adversarial Denoising Autoencoder for Face Completion</a><br>
		Avery Allen, Wenchen Li</li>
		<li><a href="https://www.google.com/url?q=https://arxiv.org/abs/1704.05420">Diagonal RNNs in Symbolic Music Modeling</a><br>
		Ellery Wulczyn, Clementine Jacoby</li>
		<li>Long Short-Term Memory<br><span style="font-style:italic">Neural Computation, 9(8):1735&ndash;1780, 1997</span><br>
		S. Hochreiter and J. Schmidhuber</li>
		<li><a href="https://www.google.com/url?q=http://clementinejacoby.com/softmax_rnn_224.pdf">Softmax RNN for Short Text Classification</a><br>
		Ellery Wulczyn, Clementine Jacoby</li>
		<li><a href="https://www.google.com/url?q=http://cs229.stanford.edu/proj2015/054_report.pdf">Incorporating Nesterov Momentum into Adam</a><br>
		Timothy Dozat</li>
		<li><a href="https://www.google.com/url?q=https://arxiv.org/abs/1409.3215">Sequence to Sequence Learning with Neural Networks</a><br>
		Ilya Sutskever, Oriol Vinyals, Quoc V. Le</li>
		<li><a href="https://www.google.com/url?q=https://arxiv.org/abs/1609.09106">HyperNetworks</a><br>
		David Ha, Andrew Dai, Quoc V. Le</li>
		<li><a href="https://www.google.com/url?q=https://publications.aston.ac.uk/373/1/NCRG_94_004.pdf">Mixture Density Networks</a><br>
		Christopher M. Bishop</li>
		<li><a href="https://www.google.com/url?q=http://www.etsi.org/deliver/etsi_gts/06/0610/03.02.00_60/gsmts_0610sv030200p.pdf">GSM 06.10 &ldquo;Full Rate Speech Transcoding&rdquo;</a></li>
		<li><a href="https://www.google.com/url?q=http://www.rowetel.com/?page_id%3D452">Codec 2 website</a><br>
		David Rowe</li>
		<li><a href="https://github.com/philayres/babble-rnn">Babbler-rnn c2gen Git repository on Github</a></li>
	</ol>
	
	
	<h2>Additional Resources</h2>
	<h3 id="h.gfev4o4f20r1">Codec 2 Data Frame</h3>
	<p>For reference, each frame of Codec 2 data is represented as:</p>
	<a id="t.14a040d1a00c91f27a629d49514fce9bfa7b79e4"></a><a id="t.0"></a>
	<table style="margin-left:;border-spacing:0;border-collapse:collapse;margin-right:auto">
		<tbody>
			<tr style="height:0pt">
				<td colspan="1" rowspan="1" style="border-right-style:solid;padding:5pt 5pt 5pt 5pt;border-bottom-border-top-width:1pt;border-right-width:1pt;border-left-vertical-align:top;border-right-border-left-width:1pt;border-top-style:solid;border-left-style:solid;border-bottom-width:1pt;width:225.7pt;border-top-border-bottom-style:solid">
					<p><span style="font-weight:700;text-decoration:none;vertical-align:baseline;;font-style:normal">number of bits per parameter</span></p>
				</td>
				<td colspan="1" rowspan="1" style="border-right-style:solid;padding:5pt 5pt 5pt 5pt;border-bottom-border-top-width:1pt;border-right-width:1pt;border-left-vertical-align:top;border-right-border-left-width:1pt;border-top-style:solid;border-left-style:solid;border-bottom-width:1pt;width:225.7pt;border-top-border-bottom-style:solid">
					<p style="padding:0;margin:0;;line-height:1.0;text-align:left"><span style="font-weight:700;text-decoration:none;vertical-align:baseline;;font-style:normal">parameter type</span></p>
				</td>
			</tr>
			<tr style="height:0pt">
				<td colspan="1" rowspan="1" style="border-right-style:solid;padding:5pt 5pt 5pt 5pt;border-bottom-border-top-width:1pt;border-right-width:1pt;border-left-vertical-align:top;border-right-border-left-width:1pt;border-top-style:solid;border-left-style:solid;border-bottom-width:1pt;width:225.7pt;border-top-border-bottom-style:solid">
					<p>1,1,1,1</p>
				</td>
				<td colspan="1" rowspan="1" style="border-right-style:solid;padding:5pt 5pt 5pt 5pt;border-bottom-border-top-width:1pt;border-right-width:1pt;border-left-vertical-align:top;border-right-border-left-width:1pt;border-top-style:solid;border-left-style:solid;border-bottom-width:1pt;width:225.7pt;border-top-border-bottom-style:solid">
					<p>voiced flags for 4 (10ms) PCM &lsquo;frames&rsquo;</p>
				</td>
			</tr>
			<tr style="height:0pt">
				<td colspan="1" rowspan="1" style="border-right-style:solid;padding:5pt 5pt 5pt 5pt;border-bottom-border-top-width:1pt;border-right-width:1pt;border-left-vertical-align:top;border-right-border-left-width:1pt;border-top-style:solid;border-left-style:solid;border-bottom-width:1pt;width:225.7pt;border-top-border-bottom-style:solid">
					<p style="padding:0;margin:0;;line-height:1.0;text-align:left">7</p>
				</td>
				<td colspan="1" rowspan="1" style="border-right-style:solid;padding:5pt 5pt 5pt 5pt;border-bottom-border-top-width:1pt;border-right-width:1pt;border-left-vertical-align:top;border-right-border-left-width:1pt;border-top-style:solid;border-left-style:solid;border-bottom-width:1pt;width:225.7pt;border-top-border-bottom-style:solid">
					<p style="padding:0;margin:0;;line-height:1.0;text-align:left">W<span style="vertical-align:sub">0</span>&nbsp;(fundamental frequency, &ldquo;pitch&rdquo;)</p>
				</td>
			</tr>
			<tr style="height:0pt">
				<td colspan="1" rowspan="1" style="border-right-style:solid;padding:5pt 5pt 5pt 5pt;border-bottom-border-top-width:1pt;border-right-width:1pt;border-left-vertical-align:top;border-right-border-left-width:1pt;border-top-style:solid;border-left-style:solid;border-bottom-width:1pt;width:225.7pt;border-top-border-bottom-style:solid">
					<p style="padding:0;margin:0;;line-height:1.0;text-align:left">5</p>
				</td>
				<td colspan="1" rowspan="1" style="border-right-style:solid;padding:5pt 5pt 5pt 5pt;border-bottom-border-top-width:1pt;border-right-width:1pt;border-left-vertical-align:top;border-right-border-left-width:1pt;border-top-style:solid;border-left-style:solid;border-bottom-width:1pt;width:225.7pt;border-top-border-bottom-style:solid">
					<p style="padding:0;margin:0;;line-height:1.0;text-align:left">E (energy of W<span style="vertical-align:sub">0</span>)</p>
				</td>
			</tr>
			<tr style="height:0pt">
				<td colspan="1" rowspan="1" style="border-right-style:solid;padding:5pt 5pt 5pt 5pt;border-bottom-border-top-width:1pt;border-right-width:1pt;border-left-vertical-align:top;border-right-border-left-width:1pt;border-top-style:solid;border-left-style:solid;border-bottom-width:1pt;width:225.7pt;border-top-border-bottom-style:solid">
					<p>4,4,4,4,4,4,4,3,3,2</p>
				</td>
				<td colspan="1" rowspan="1" style="border-right-style:solid;padding:5pt 5pt 5pt 5pt;border-bottom-border-top-width:1pt;border-right-width:1pt;border-left-vertical-align:top;border-right-border-left-width:1pt;border-top-style:solid;border-left-style:solid;border-bottom-width:1pt;width:225.7pt;border-top-border-bottom-style:solid">
					<p style="padding:0;margin:0;;line-height:1.0;text-align:left">LSP (spectral magnitudes)</p>
				</td>
			</tr>
		</tbody>
	</table>
	
	<h3 id="h.1c0d9jetrhe2">Codec 2 Parameter Progression</h3>
	<p>A rapid graphical evaluation of the progression from one Codec 2 frame to the next was performed. The objective was to see the likely impact on audio output based on small errors in each value.</p>
	
	<p>The charts each show a single Codec 2 parameter over a significant sample of the corpus. The charts show current value on the horizontal axis, against next value on vertical axis. This rapid visualisation shows how the progression from one frame to the next is generally clustered, although varies between parameters.</p>
	
	<p><span style="font-weight:700;text-decoration:none;vertical-align:baseline;;font-style:normal">Fundamental frequency</span></p>
	<p>The strong cluster around <span style="font-style:italic">x=y</span> follows an expectation that the primary vocal pitch does not rapidly fluctuate over time. Interestingly, at the lower end there is more variation.</p>
	<p style="text-align:center"><img src="codec-plot-4.png" /></p>
	<p><span style="font-weight:700;text-decoration:none;vertical-align:baseline;;font-style:normal">Energy</span></p>
	<p>Variations in energy vary more at the top end of the range. It is not expected that errors here will lead to incomprehensible speech.</p>
		<p style="text-align:center"><img src="codec-plot-5.png" /></p>
	<p><span style="font-weight:700;text-decoration:none;vertical-align:baseline;;font-style:normal">Spectral magnitudes</span></p>
	<p>These vary significantly from one frame to the next. The parameters with the greatest range again follow <span style="font-style:italic">x = y</span> to a limited degree. Lower range parameters appear to have a greater spread of next values.</p>
		<p style="text-align:center"><img src="codec-plot-6.png" /></p>
		<p style="text-align:center"><img src="codec-plot-7.png" /></p>
		<p style="text-align:center"><img src="codec-plot-8.png" /></p>
		<p style="text-align:center"><img src="codec-plot-9.png" /></p>
		<p style="text-align:center"><img src="codec-plot-10.png" /></p>
		<p style="text-align:center"><img src="codec-plot-11.png" /></p>
		<p style="text-align:center"><img src="codec-plot-12.png" /></p>
 	 <p style="text-align:center"><img src="codec-plot-13.png" /></p>
		<p style="text-align:center"><img src="codec-plot-14.png" /></p>
		<p style="text-align:center"><img src="codec-plot-15.png" /></p>
											
	<p><span style="font-weight:700;text-decoration:none;vertical-align:baseline;;font-style:normal">&ldquo;Voiced&rdquo; parameters</span></p>
	<p>These do not show any consistent progression over time. Only one of the parameter charts is shown as they appear identical.</p>
		<p style="text-align:center"><img src="codec-plot-0.png" /></p>
		
			<h2 id="additionalresults">Additional Generated Audio</h2>
			<p>60 seconds of audio generated by a 3 x 320 unit LSTM. The initial 8 seconds are seed data from the original corpus.</p>
			<audio controls>
 <source src="longbabble505a.wav" type="audio/wav">
 Your browser does not support the audio tag.
</audio>
			<p>60 seconds of audio generated by a 3 x 160 unit LSTM. The initial 8 seconds are seed data from the original corpus.</p>
<audio controls>
 <source src="longbabble540a.wav" type="audio/wav">
 Your browser does not support the audio tag.
</audio>

<p><a href="#top">return to the introduction</a></p>
<p><a href="https://github.com/philayres/babble-rnn">view project source code on GitHub</a></p>

<div id="disqus_thread"></div>
<script>
var disqus_config = function () {
this.page.url = "http://babble-rnn.consected.com/docs/babble-rnn-generating-speech-from-speech-post.html";  
this.page.identifier = "babble-rnn-generating-speech-from-speech-post"; 
};

(function() { // DON'T EDIT BELOW THIS LINE
var d = document, s = d.createElement('script');
s.src = 'https://babble-rnn.disqus.com/embed.js';
s.setAttribute('data-timestamp', +new Date());
(d.head || d.body).appendChild(s);
})();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
                                
</div>
</body>
</html>